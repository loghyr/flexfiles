<!-- Copyright (C) The IETF Trust (2014) -->
<!-- Copyright (C) The Internet Society (2014) -->

<section title="Flexible File Layout Type" anchor="sec:flexLayout">
  <t>
    The layout4 type is defined in <xref target="RFC5662" /> as follows:
  </t>

  <figure>
    <artwork>
    enum layouttype4 {
        LAYOUT4_NFSV4_1_FILES   = 1,
        LAYOUT4_OSD2_OBJECTS    = 2,
        LAYOUT4_BLOCK_VOLUME    = 3,
        LAYOUT4_FLEX_FILES      = 4
[[RFC Editor: please modify the LAYOUT4_FLEX_FILES
  to be the layouttype assigned by IANA]]
    };
   
    struct layout_content4 {
        layouttype4             loc_type;
        opaque                  loc_body&lt;&gt;;
    };
   
    struct layout4 {
        offset4                 lo_offset;
        length4                 lo_length;
        layoutiomode4           lo_iomode;
        layout_content4         lo_content;
    };
    </artwork>
   </figure>

   <t>
     This document defines structure associated with the layouttype4
     value LAYOUT4_FLEX_FILES.  <xref target="RFC5661" />
     specifies the loc_body structure as an XDR type "opaque".  The
     opaque layout is uninterpreted by the generic pNFS client
     layers, but obviously must be interpreted by the Flexible File
     Layout Type driver.  This section defines the structure of this
     opaque value, pnfs_ff_layout4.

     <cref anchor='AI1 ' source='TH'>
       What is a driver? I.e., I know what it is, but have we
       defined it? Are we talking implementation here?
     </cref>
   </t>

   <section title="pnfs_ff_layout" anchor="pnfs_ff_layout">
     <figure>
       <artwork>
/// enum pnfs_ff_striping_pattern {
///     PFSP_SPARSE_STRIPING = 1,
///     PFSP_DENSE_STRIPING  = 2,
///     PFSP_RAID_4          = 4,
///     PFSP_RAID_5          = 5,
///     PFSP_RAID_PQ         = 6
/// };
///
/// enum pnfs_ff_comp_type {
///     PNFS_FF_COMP_MISSING = 0,
///     PNFS_FF_COMP_PACKED  = 1,
///     PNFS_FF_COMP_FULL    = 2
/// };
///
/// struct pnfs_ff_comp_full {
///     deviceid4               pfcf_deviceid;
///     nfs_fh4                 pfcf_fhandle;
///     stateid4                pfcf_stateid;
///     opaque_auth             pfcf_auth;
///     uint32_t                pfcf_metric;
/// };
///
/// union pnfs_ff_comp switch (pnfs_ff_comp_type pfc_type) {
///    case PNFS_FF_COMP_MISSING:
///         void;
///
///    case PNFS_FF_COMP_PACKED:
///         deviceid4               pfcp_deviceid;
///
///    case PNFS_FF_COMP_FULL:
///         pnfs_ff_comp_full       pfcp_full;
/// };
///
/// struct pnfs_ff_layout {
///     pnfs_ff_striping_pattern    pfl_striping_pattern;
///     uint32_t                    pfl_num_comps;
///     uint32_t                    pfl_mirror_cnt;
///     length4                     pfl_stripe_unit;
///     nfs_fh4                     pfl_global_fh;
///     uint32_t                    pfl_comps_index;
///     pnfs_ff_comp                pfl_comps&lt;&gt;;
/// };
///
       </artwork>
     </figure>

     <t>
       The pnfs_ff_layout structure specifies a layout over a set
       of Component Objects.  The layout parameterizes the algorithm
       that maps the file's contents within the returned byte range,
       as represented by lo_offset and lo_length, over the Component
       Objects.
     </t>

     <t>
       It is possible that the file is concatenated from more than
       one layout segment.  Each layout segment MAY represent different
       striping parameters, applying respectively only to the layout
       segment byte range.
     </t>

     <t>
       This section provides a brief introduction to the layout
       parameters.  See <xref target="sec:stripingTopologies" /> for
       a more detailed description of the different striping schemes
       and the respective interpretation of the layout parameters
       for each striping scheme.
     </t>

     <t>
       In addition to mapping data using simple striping schemes
       where loss of a single component object results in data loss,
       the layout parameters support mirroring and more advanced
       redundancy schemes that protect against loss of component
       objects.  pfl_striping_pattern represents the algorithm to
       be used for mapping byte offsets in the file address space
       to corresponding component objects in the returned layout and
       byte offsets in the component's address space.  pfl_striping_pattern
       also represents methods for storing and retrieving redundant
       data that can be used to recover from failure or loss of
       component objects.
     </t>

     <t>
       pfl_num_comps is the total number of component objects the
       file is striped over within the returned byte range, not
       counting mirrored components (See pfl_mirror_cnt below).  Note
       that the server MAY grow the file by adding more components
       to the stripe while clients hold valid layouts until the file
       has reached its final stripe width.
     </t>

     <t>
       pfl_mirror_cnt represents the number of mirrors each component
       in the stripe has.  If there is no mirroring then pfm_mirror_cnt
       MUST be 0.  Otherwise, the number of entries listed in pfl_comps
       MUST be a multiple of (pfl_mirror_cnt + 1).
     </t>

     <t>
       pfl_stripe_unit is the number of bytes placed on one component
       before advancing to the next one in the list of components.
       When the file is striped over a single component object
       (pfl_num_comps equals to 1), the stripe unit has no use and
       the server SHOULD set it to the server default value or to
       zero; otherwise, pfl_stripe_unit MUST NOT be set to zero.
     </t>

     <t>
       The pfl_comps field represents an array of component objects.
       The data placement algorithm that maps file data onto component
       objects assumes that each component object occurs exactly
       once in the array of components.  Therefore, component objects
       MUST appear in the pfl_comps array only once.  The components
       array may represent all objects comprising the file, in which
       case pfl_comps_index is set to zero and the number of entries
       in the pfl_comps array is equal to pfl_num_comps * (pfl_mirror_cnt
       + 1).  The server MAY return fewer components than pfl_num_comps,
       provided that the returned byte range represented by lo_offset
       and lo_count maps in whole into the set of returned component
       objects.  In this case, pfl_comps_index represents the logical
       position of the returned components array, pfl_comps, within
       the full array of components that comprise the file.
       pfl_comps_index MUST be a multiple of (pfl_mirror_cnt + 1).
     </t>

     <t>
       Each component object in the pfl_comps array is described by the
       pnfs_ff_comp type.
     </t>

     <t>
       When a component object is unavailable pfc_type is set to
       PNFS_FF_COMP_MISSING and no other information for this component
       is returned.  When a data redundancy scheme is being used,
       as represented by pfl_striping_pattern, the client MAY use a
       respective data recovery algorithm to reconstruct data that
       is logically stored on the missing component using user data
       and redundant data stored on the available components in the
       containing stripe.
     </t>

     <t>
       The server MUST set the same pfc_type for all available components to
       either PNFS_FF_COMP_PACKED or PNFS_FF_COMP_FULL.
     </t>

     <t>
       When NFSv4.1 Clustered Data Servers are used, the metadata
       server implements the global state model where all data servers
       share the same stateid and filehandle for the file.  In such
       case, the client MUST use the open, delegation, or lock stateid
       returned by the metadata server for the file for accessing
       the Data Servers for READ and WRITE; the global filehandle
       to be used by the client is provided by pfl_global_fh.  If
       the metadata server filehandle for the file is being used by
       all data servers then pfl_global_fh MAY be set to an empty
       filehandle.
     </t>

     <t>
       pfcp_deviceid or pfcf_deviceid provide the
       deviceid of the data server holding the Component Object.
     </t>

     <t>
       When standalone data servers are used, either over NFSv4 or
       NFSv4.1, pfl_global_fh SHOULD be set to an empty filehandle
       and it MUST be ignored by the client and pfcf_fhandle provides
       the filehandle of the Data Server file holding the Component
       Object, and pfcf_stateid provides the stateid to be used by
       the client to access the file.
     </t>

     <t>
       For NFSv3 Data Servers, pfcf_auth provides the RPC credentials
       to be used by the client to access the Component Objects.
       For NFSv4.x Data Servers, the server SHOULD use the AUTH_NONE
       flavor and a zero length opaque body to minimize the returned
       structure length.  The client MUST ignore pfxf_auth in this case.
     </t>

     <t>
       When pfl_mirror_cnt is not zero pfcf_metric indicates the
       distance to the client the distance of the respective component
       object, otherwise the server MUST set pfcf_metric to zero.
       When reading data, the client the client is advised to read
       from components with the lowest pfcf_metric.  When there are
       several components with the same pfcf_metric client implementations
       may implement a load distribution algorithm to evenly distribute
       the read load across several devices and by so provide larger
       bandwidth.
     </t>
   </section>

   <section title="Striping Topologies" anchor="sec:stripingTopologies">
     <t>
       This section describes the different data mapping schemes in detail.
     </t>

     <t>
       pnfs_ff_striping_pattern determines the algorithm and placement
       of redundant data.  This section defines the different
       redundancy algorithms.  Note: The term "RAID" (Redundant Array
       of Independent Disks) is used in this document to represent
       an array of Component Objects that store data for an individual
       User File.  The objects are stored on independent Data Servers.
       User File data is encoded and striped across the array of
       Component Objects using algorithms developed for block-based
       RAID systems.
     </t>

     <section anchor="sec:sparseStriping" title="PFSP_SPARSE_STRIPING">
       <t>
         The mapping from the logical offset within a file (L)
         to the Component Object C and object-specific offset
         O is direct and straight forward as defined by the
         following equations:
       </t>

       <figure>
         <artwork>
L: logical offset into the file

W: stripe width
    W = pfl_num_comps

S: number of bytes in a stripe
    S = W * pfl_stripe_unit

N: stripe number
    N = L / S

C: component index corresponding to L
   C = (L % S) / pfl_stripe_unit

O: The component offset corresponding to L
   O = L
         </artwork>
       </figure>

       <t>
         Note that this computation does not accommodate the same
         object appearing in the pfl_comps array multiple times.
         Therefore the server may not return layouts with the same
         object appearing multiple times. If needed the server can
         return multiple layout segments each covering a single
         instance of the object.
       </t>

       <t>
         PFSP_SPARSE_STRIPING means there is no
         parity data, so all bytes in the component objects are
         data bytes located by the above equations for C and O.
         If a component object is marked as PNFS_FF_COMP_MISSING,
         the pNFS client MUST either return an I/O error if this component
         is attempted to be read or, alternatively, it can
         retry the READ against the pNFS server.
       </t>
     </section>

    <section anchor="sec:denseStriping" title="PFSP_DENSE_STRIPING">
      <t>
        The mapping from the logical
        offset within a file (L) to the component object C and
        object-specific offset O is defined by the following equations:
      </t>

      <figure>
        <artwork>
L: logical offset into the file

W: stripe width
    W = pfl_num_comps

S: number of bytes in a stripe
    S = W * pfl_stripe_unit

N: stripe number
    N = L / S

C: component index corresponding to L
   C = (L % S) / pfl_stripe_unit

O: The component offset corresponding to L
   O = (N * pfl_stripe_unit) + (L % pfl_stripe_unit)
        </artwork>
      </figure>

      <t>
        Note that this computation does not accommodate the same
        object appearing in the pfl_comps array multiple times.
        Therefore the server may not return layouts with the same
        object appearing multiple times. If needed the server can
        return multiple layout segments each covering a single
        instance of the object.
      </t>

      <t>
        PFSP_DENSE_STRIPING means there is no parity data, so all
        bytes in the component objects are data bytes located by
        the above equations for C and O.  If a component object is
        marked as PNFS_FF_COMP_MISSING, the pNFS client MUST either
        return an I/O error if this component is attempted to be
        read or, alternatively, it can retry the READ against the
        pNFS server.
      </t>

      <t>
        Note that the layout depends on the file size, which the
        client learns from the generic return parameters of LAYOUTGET,
        by doing GETATTR commands to the Metadata Server.  The
        client uses the file size to decide if it should fill holes
        with zeros or return a short read.  Striping patterns can
        cause cases where Component Objects are shorter than other
        components because a hole happens to correspond to the last
        part of the Component Object.
      </t>
    </section>

    <section anchor="PFSP_RAID_4" title="PFSP_RAID_4">
      <t>
        PFSP_RAID_4 means that the last component object in the
        stripe contains parity information computed over the rest
        of the stripe with an XOR operation.  If a Component Object
        is unavailable, the client can read the rest of the stripe
        units in the damaged stripe and recompute the missing stripe
        unit by XORing the other stripe units in the stripe.  Or
        the client can replay the READ against the pNFS server that
        will presumably perform the reconstructed read on the
        client's behalf.
      </t>

      <t>
        When parity is present in the file, then the number of
        parity devices is taken into account in the above equations
        when calculating (D), the number of data devices in a stripe,
        as follows:
      </t>

      <figure>
        <artwork>
P: number of parity devices in each stripe
   P = 1

D: number of data devices in a stripe
   D = W - P

I: parity device index
   I = D
        </artwork>
      </figure>
    </section>

    <section anchor="PFSP_RAID_5" title="PFSP_RAID_5">
      <t>
        PNFS_OBJ_RAID_5 means that the position of the parity data is
        rotated on each stripe.  In the first stripe, the last component
        holds the parity.  In the second stripe, the next-to-last
        component holds the parity, and so on.  In this scheme, all
        stripe units are rotated so that I/O is evenly spread across
        objects as the file is read sequentially.  The rotated parity
        layout is illustrated here, with hexadecimal numbers indicating
        the stripe unit.
      </t>

      <figure>
        <artwork>
0 1 2 P
4 5 P 3
8 P 6 7
P 9 a b
        </artwork>
      </figure>

      <t>
        Note that the math for RAID_5 is similar to RAID_4 only
        that the device indices for each stripe are rotated backwards.
        So start with the equations above for RAID_4, then compute
        the rotation as described below.
      </t>

      <figure>
        <artwork>
P: number of parity devices in each stripe
   P = 1

PC: Parity Cycle
    PC = W

R: The parity rotation index
   (N is as computed in above equations for RAID-4)
   R = N % PC

I: parity device index
   I = (W + W - (R + 1) * P) % W

Cr: The rotated device index
    (C is as computed in the above equations for RAID-4)
    Cr = (W + C - (R * P)) % W

Note: W is added above to avoid negative numbers modulo math.
        </artwork>
      </figure>
    </section>

    <section anchor="PFSP_RAID_PQ" title="PFSP_RAID_PQ">
      <t>
        PFSP_RAID_PQ is a double-parity scheme that uses the Reed-Solomon
        P+Q encoding scheme <xref target='MacWilliams77' />.  In
        this layout, the last two component objects hold the P and Q
        data, respectively.  P is parity computed with XOR.  The Q
        computation is described in detail in <xref target='Anvin09' />.
        The same polynomial "x^8+x^4+x^3+x^2+1" and Galois field
        size of 2^8 are used here.  Clients may simply choose to read
        data through the metadata server if two or more components are
        missing or damaged.
      </t>

      <t>
        The equations given above for embedded parity can be used to
        map a file offset to the correct component object by setting
        the number of parity components (P) to 2 instead of 1 for RAID-5
        and computing the Parity Cycle length as the Lowest Common
        Multiple of pfl_num_comps and P, divided by P, as described
        below.  Note: This algorithm can be used also for RAID-5 where
        P=1.
      </t>

      <figure>
        <artwork>
P: number of parity devices
   P = 2

PC: Parity cycle:
    PC = LCM(W, P) / P

Q: The device index holding the Q component
   (I is as computed in the above equations for RAID-5)
   Qdev = (I + 1) % W
        </artwork>
      </figure>
    </section>

    <section title="RAID Usage and Implementation Notes">
      <t>
        RAID layouts with redundant data in their stripes require
        additional serialization of updates to ensure correct operation.
        Otherwise, if two clients simultaneously write to the same
        logical range of an object, the result could include different
        data in the same ranges of mirrored tuples, or corrupt parity
        information.  It is the responsibility of the metadata server
        to enforce serialization requirements such as this. For example,
        the metadata server may do so by not granting overlapping write
        layouts within mirrored objects.
      </t>

      <t>
        Many alternative encoding schemes exist for P &gt;= 2
        <xref target='Plank07' />.  These involve P or Q
        equations different than those used in PFSP_RAID_PQ.  Thus, if
        one of these schemes is to be used in the future, a distinct
        value must be added to pnfs_ff_striping_pattern for it. While
        Reed-Solomon codes are well understood, recently discovered
        schemes such as Liberation codes are more computationally
        efficient for small group_widths, and Cauchy Reed-Solomon codes
        are more computationally efficient for higher values of P.
      </t>
    </section>
  </section>

  <section anchor="Mirroring" title="Mirroring">
    <t>
      The pfl_mirror_cnt is used to replicate a file by replicating
      its Component Objects.  If there is no mirroring, then
      pfs_mirror_cnt MUST be 0.  If pfl_mirror_cnt is greater than
      zero, then the size of the pfl_comps array MUST be a multiple
      of (pfl_mirror_cnt + 1).  Thus, for a classic mirror on two
      objects, pfl_mirror_cnt is one.  Note that mirroring can be
      defined over any striping pattern.
    </t>

    <t>
      Replicas are adjacent in the olo_components array, and the
      value C produced by the above equations is not a direct index
      into the pfl_comps array.  Instead, the following equations
      determine the replica component index RCi, where i ranges
      from 0 to pfl_mirror_cnt.
    </t>

    <figure>
      <artwork>
FW = size of pfl_comps array / (pfl_mirror_cnt+1)

C = component index for striping or two-level striping
    as calculated using above equations

i ranges from 0 to pfl_mirror_cnt, inclusive
RCi = C * (pfl_mirror_cnt+1) + i
      </artwork>
    </figure>
  </section>
</section>
