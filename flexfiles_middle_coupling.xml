<!-- Copyright (C) The IETF Trust (2014) -->
<!-- Copyright (C) The Internet Society (2014) -->

<section title="Coupling of Storage Devices">
  <t>
    A server implementation may choose either a loose or tight coupling
    model between the metadata server and the storage devices. 
    <xref target='pNFSLayouts' /> describes the general problems
    facing pNFS implementations.  This document details how
    the new Flexible File Layout Type addresses these issues.
    To implement the tight coupling model, a control protocol has to be
    defined. As the flex file layout imposes no special requirements
    on the client, the control protocol will need to provide:

    <list style='format (%d)'>
      <t>
        for the management of both security and LAYOUTCOMMITs, and,
      </t>

      <t>
        a global stateid model and management of these stateids.
      </t>
    </list>

    When implementing the loose coupling model, the only control protocol
    will be a version of NFS, with no ability to provide a global stateid
    model or to prevent clients from using layouts inappropriately. To
    enable client use in that environment, this document will specify
    how security, state, and locking are to be managed.
  </t>

  <section anchor='sec:layoutcommit' title='LAYOUTCOMMIT'>
    <t>
      Regardless of the coupling model, the metadata server has the
      responsibility, upon receiving a LAYOUTCOMMIT (see Section
      18.42 of <xref target='RFC5661' />), of ensuring that the
      semantics of pNFS are respected (see Section 3.1 of <xref
      target='pNFSLayouts' />).  These do include a requirement that
      data written to data storage device be stable before the
      occurrence of the LAYOUTCOMMIT.
    </t>

    <t>
      It is the responsibility of the client to make sure the data
      file is stable before the metadata server begins to query the
      storage devices about the changes to the file. If any WRITE to a
      storage device did not result with stable_how equal to FILE_SYNC,
      a LAYOUTCOMMIT to the metadata server MUST be preceded by a COMMIT
      to the storage devices written to.  Note that if the client has not
      done a COMMIT to the storage device, then the LAYOUTCOMMIT might not
      be synchronized to the last WRITE operation to the storage device.
    </t>
  </section>

  <section anchor='sec:sec_models' title="Fencing Clients from the Storage Device">
    <t>

      With loosely coupled storage devices, the metadata server uses
      synthetic uids (user ids) and gids (group ids) for the data file,
      where the uid owner of the data file is allowed read/write access
      and the gid owner is allowed read only access.  As part of the
      layout (see ffds_user and ffds_group in  <xref target='ff_layout4' />),
      the client is provided with the user and group to be used in
      the Remote Procedure Call (RPC) <xref target='RFC5531' />
      credentials needed to access the data file.  Fencing off of
      clients is achieved by the metadata server changing the synthetic
      uid and/or gid owners of the data file on the storage device
      to implicitly revoke the outstanding RPC credentials. A client
      presenting the wrong credential for the desired access will get
      a NFS4ERR_ACCESS error.

    </t>

    <t>
      With this loosely coupled model, the metadata server is 
      not able to fence off a single client, it is forced to fence
      off all clients. However, as the other clients react to
      the fencing, returning their layouts and trying to get
      new ones, the metadata server can hand out a new uid and
      gid to allow access.
    </t>

    <t>
      Note: it is recommended to implement common access control
      methods at the storage device filesystem to
      allow only the metadata server root (super user) access to
      the storage device, and to set the owner of all directories
      holding data files to the root user.  This approach
      provides a practical model to enforce access control and fence off
      cooperative clients, but it can not protect against malicious
      clients; hence it provides a level of security equivalent to
      AUTH_SYS.
    </t>

    <t>
      With tightly coupled storage devices, the metadata server
      sets the user and group owners, mode bits, and ACL of the
      data file to be the same as the metadata file. And the client
      must authenticate with the storage device and go through the
      same authorization process it would go through via the metadata
      server.  In the case of tight coupling, fencing is the
      responsibility of the control protocol and is not described in
      detail here.  However, implementations of the tight coupling locking
      model (see <xref target='ss:locking' />), will need a way to prevent
      access by certain clients to specific files by invalidating the
      corresponding stateids on the storage device. In such a scenario,
      the client will be given an error of NFS4ERR_BAD_STATEID.
    </t>

    <t>
      The client need not know the model used between the metadata
      server and the storage device. It need only react consistently
      to any errors in interacting with the storage device. It should
      both return the layout and error to the metadata server and ask for
      a new layout. At that point, the metadata server can either
      hand out a new layout, hand out no layout (forcing the I/O through
      it), or deny the client further access to the file.
    </t>

    <section title='Implementation Notes for Synthetic uids/gids'>
      <t>
        The selection method for the synthetic uids and gids to
        be used for fencing in loosely coupled storage devices
        is strictly an implementation issue. I.e.,
        an administrator might restrict a range of such
        ids available to the Lightweight Directory
        Access Protocol (LDAP) 'uid' field <xref target='RFC4519' />.
        She might also be able to
        choose an id that would never be used to grant access.
        Then when the metadata server had a request to access
        a file, a SETATTR would be sent to the storage device
        to set the owner and group of the data file. The user
        and group might be selected in a round robin fashion
        from the range of available ids.
      </t>

      <t>
        Those ids would be sent back as ffds_user and ffds_group
        to the client. And it would present them as the RPC
        credentials to the storage device. When the client was
        done accessing the file and the metadata server knew
        that no other client was accessing the file, it could
        reset the owner and group to restrict access to the
        data file.
      </t>

      <t>
        When the metadata server wanted to fence off a client,
        it would change the synthetic uid and/or gid to
        the restricted ids. Note that using a restricted id
        ensures that there is a change of owner and at least
        one id available that never gets allowed access.
      </t>

      <t>
        Under an AUTH_SYS security model, synthetic uids and gids of 0
        SHOULD be avoided. These typically either grant super access to
        files on a storage device or are mapped to an anonymous id.
        In the first case, even if the data file is fenced, the client
        might still be able to access the file. In the second case,
        multiple ids might be mapped to the anonymous ids.
      </t>
    </section>

    <section title='Example of using Synthetic uids/gids'>
      <t>
        The user loghyr creates a file "ompha.c" on the metadata
        server and it creates a corresponding data file on the
        storage device.
      </t>

      <t>
        The metadata server entry may look like:
      </t>

      <figure>
        <artwork>
-rw-r--r--    1 loghyr  staff    1697 Dec  4 11:31 ompha.c
        </artwork>
      </figure>

      <t>
        On the storage device, it may be assigned some
        random synthetic uid/gid to deny access:
      </t>

      <figure>
        <artwork>
-rw-r-----    1 19452   28418    1697 Dec  4 11:31 data_ompha.c
        </artwork>
      </figure>

      <t>
        When the file is opened on a client and accessed, it will try to get a
        layout for the data file. Since the layout knows nothing
        about the user (and does not care), whether the user loghyr or
        garbo opens the file does not matter.  The client has to present
        an uid of 19452 to get write permission. If it presents any other
        value for the uid, then it must give a gid of 28418 to get read
        access.
      </t>

      <t>
        Further, if the metadata server decides to fence the file, it may
        change the uid and gid as such:
      </t>

      <figure>
        <artwork>
-rw-r-----    1 19453   28419    1697 Dec  4 11:31 data_ompha.c
        </artwork>
      </figure>

      <t>
        The set of synthetic gids on the storage device
        should be selected such that there is no mapping in any of
        the name services used by the storage device. I.e., each
        group should have no members.
      </t>

      <t>
        If the layout segment has an iomode of LAYOUTIOMODE4_READ, then
        the metadata server should return a synthetic uid that is
        not set on the storage device. Only the synthetic gid would
        be valid.
      </t>

      <t>
        The client is thus solely responsible for enforcing file
        permissions in a loosely coupled model.  To allow loghyr
        write access, it will send an RPC to the storage device
        with a credential of 1066:1067.  To allow garbo read access,
        it will send an RPC to the storage device with a credential
        of 1067:1067. The value of the uid does not matter as
        long as it is not the synthetic uid granted it when getting
        the layout.
      </t>

      <t>
        While pushing the enforcement of permission checking onto
        the client may seem to weaken security, the client may already
        be responsible for enforcing permissions before modifications
        are sent to a server. With cached writes, the client is
        always responsible for tracking who is modifying a file and
        making sure to not coalesce requests from multiple users
        into one request.
      </t>
    </section>
  </section>

  <section anchor="ss:locking" title="State and Locking Models">
    <t>
      An implementation can always be deployed as a loosely
      coupled model. There is however no way for a storage
      device to indicate over a NFS protocol that it can
      definitively participate in a tightly coupled model:

      <list style='symbols'>
        <t>
          Storage devices implementing the NFSv3 and NFSv4.0 protocols
          are always treated as loosely coupled.
        </t>

        <t>
          NFSv4.1+ storage devices that do not return the
          EXCHGID4_FLAG_USE_PNFS_DS flag set to EXCHANGE_ID are indicating
          that they are to be treated as loosely coupled.  From the
          locking viewpoint they are treated in the same way as NFSv4.0
          storage devices.
        </t>

        <t>
          NFSv4.1+ storage devices that do identify themselves with the
          EXCHGID4_FLAG_USE_PNFS_DS flag set to EXCHANGE_ID can potentially
          be tightly coupled.  They would use a back-end control protocol
          to implement the global stateid model as described in
          <xref target="RFC5661" />.
        </t>
      </list>

      A storage device would have to either be discovered or
      advertised over the control protocol to enable a tight
      coupling model.
    </t>

    <section title='Loosely Coupled Locking Model'>
      <t>
        When locking-related operations are requested, they are primarily
        dealt with by the metadata server, which generates the appropriate
        stateids.  When an NFSv4 version is used as the data access
        protocol, the metadata server may make stateid-related requests
        of the storage devices.  However, it is not required to
        do so and the resulting stateids are known only to the metadata
        server and the storage device.
      </t>

      <t>
        Given this basic structure, locking-related operations are
        handled as follows:

        <list style='symbols'>
          <t>
            OPENs are dealt with by the metadata server.  Stateids are
            selected by the metadata server and associated with the client
            id describing the client's connection to the metadata server.
            The metadata server may need to interact with the storage
            device to locate the file to be opened, but no locking-related
            functionality need be used on the storage device.
            <vspace blankLines="1" />
            OPEN_DOWNGRADE and CLOSE only require local execution on
            the metadata server.
          </t>

          <t>
            Advisory byte-range locks can be implemented locally on
            the metadata server.  As in the case of OPENs, the stateids
            associated with byte-range locks are assigned by the metadata
            server and only used on the metadata server.
          </t>

          <t>
            Delegations are assigned by the metadata server which
            initiates recalls when conflicting OPENs are processed.
            No storage device involvement is required.
          </t>

          <t>
            TEST_STATEID and FREE_STATEID are processed locally on the
            metadata server, without storage device involvement.
          </t>
        </list>
      </t>

      <t>
        All I/O operations to the storage device are done using the
        anonymous stateid.  Thus the storage device has no information
        about the openowner and lockowner responsible for issuing a
        particular I/O operation.  As a result:

        <list style='symbols'>
          <t>
            Mandatory byte-range locking cannot be supported because
            the storage device has no way of distinguishing I/O
            done on behalf of the lock owner from those done by others.
          </t>

          <t>
            Enforcement of share reservations is the responsibility of
            the client.   Even though I/O is done using the anonymous
            stateid, the client must ensure that it has a valid stateid
            associated with the openowner, that allows the I/O being done
            before issuing the I/O.
          </t>
        </list>
      </t>

      <t>
        In the event that a stateid is revoked, the metadata server is
        responsible for preventing client access, since it
        has no way of being sure that the client is aware that
        the stateid in question has been revoked.
      </t>

      <t>
        As the client never receives a stateid generated by a
        storage device, there is no client lease on the storage device
        and no prospect of lease expiration, even when access is via NFSv4
        protocols.  Clients will have leases on the metadata server.
        In dealing with lease expiration, the metadata server may need
        to use fencing to prevent revoked stateids from being relied
        upon by a client unaware of the fact that they have been revoked.
      </t>
    </section>

    <section title='Tightly Coupled Locking Model'>
      <t>
        When locking-related operations are requested, they are primarily
        dealt with by the metadata server, which generates the appropriate
        stateids.  These stateids must be made known to the storage
        device using control protocol facilities, the details of which
        are not discussed in this document.
      </t>

      <t>
        Given this basic structure, locking-related operations are
        handled as follows:

        <list style='symbols'>
          <t>
            OPENs are dealt with primarily on the metadata server.
            Stateids are selected by the metadata server and associated
            with the client id describing the client's connection to
            the metadata server.  The metadata server needs to interact
            with the storage device to locate the file to be opened,
            and to make the storage device aware of the association
            between the metadata-server-chosen stateid and the client
            and openowner that it represents.
            <vspace blankLines="1" />
            OPEN_DOWNGRADE and CLOSE are executed initially on the
            metadata server but the state change made must be propagated
            to the storage device.
          </t>

          <t>
            Advisory byte-range locks can be implemented locally on the
            metadata server.  As in the case of OPENs, the stateids
            associated with byte-range locks, are assigned by the
            metadata server and are available for use on the metadata
            server.  Because I/O operations are allowed to present lock
            stateids, the metadata server needs the ability to make the
            storage device aware of the association between the
            metadata-server-chosen stateid and the corresponding open
            stateid it is associated with.
          </t>

          <t>
            Mandatory byte-range locks can be supported when both
            the metadata server and the storage devices have
            the appropriate support.  As in the case of advisory
            byte-range locks, these are assigned by the metadata
            server and are available for use on the metadata server.
            To enable mandatory lock enforcement on the storage
            device, the metadata server needs the ability to make the
            storage device aware of the association between the
            metadata-server-chosen stateid and the client, openowner,
            and lock (i.e., lockowner, byte-range, lock-type) that it
            represents.  Because I/O operations are allowed to present
            lock stateids, this information needs to be propagated to all
            storage devices to which I/O might be directed rather than
            only to storage device that contain the locked region.
          </t>

          <t>
            Delegations are assigned by the metadata server which
            initiates recalls when conflicting OPENs are processed.
            Because I/O operations are allowed to present delegation
            stateids, the metadata server requires the ability to make
            the storage device aware of the association between
            the metadata-server-chosen stateid and the filehandle
            and delegation type it represents, and to break such an
            association.
          </t>

          <t>
            TEST_STATEID is processed locally on the metadata server,
            without storage device involvement.
          </t>

          <t>
            FREE_STATEID is processed on the metadata server but the
            metadata server requires the ability to propagate the request
            to the corresponding storage devices.
          </t>
        </list>
      </t>

      <t>
        Because the client will possess and use stateids valid on the
        storage device, there will be a client lease on the storage
        device and the possibility of lease expiration does exist.
        The best approach for the storage device is to retain these locks
        as a courtesy.  However, if it does not do so, control protocol
        facilities need to provide the means to synchronize lock state
        between the metadata server and storage device.
      </t>

      <t>
        Clients will also have leases on the metadata server, which
        are subject to expiration.  In dealing with lease expiration,
        the metadata server would be expected to use control protocol
        facilities enabling it to invalidate revoked stateids on
        the storage device.  In the event the client is not
        responsive, the metadata server may need to use fencing to prevent
        revoked stateids from being acted upon by the storage device.
      </t>
    </section>
  </section>
</section>
