<!-- Copyright (C) The IETF Trust (2014) -->
<!-- Copyright (C) The Internet Society (2014) -->

<section title="Coupling of Storage Devices">
  <t>
    The coupling of the metadata server with the storage devices
    can be either tight or loose. In a tight coupling, there is
    a control protocol present to manage security, LAYOUTCOMMITs,
    etc.  With a loose coupling, the only control protocol might
    be a version of NFS. As such, semantics for managing security,
    state, and locking models MUST be defined.
  </t>

  <t>
    A file is split into metadata and data. The "metadata file" is
    that part of the file stored on the metadata server. The "data
    file" is that part of the file stored on the storage device.
    And the "file" is the combination of the two.
  </t>

  <section anchor='sec:layoutcommit' title='LAYOUTCOMMIT'>
    <t>
      With a tightly coupled system, when the metadata server
      receives a LAYOUTCOMMIT (see Section 18.42 of
      <xref target='RFC5661' />), the semantics of the File
      Layout Type MUST be met (see Section 12.5.4 of
      <xref target='RFC5661' />). With a loosely coupled
      system, a LAYOUTCOMMIT to the metadata server MUST
      be proceeded with a COMMIT to the storage device. I.e.,
      it is the responsibility of the client to make sure
      the data file is stable before the metadata server
      begins to query the storage devices about the
      changes to the file.  Note that if the client has
      not done a COMMIT to the storage device, then the
      LAYOUTCOMMIT might not be synchronized to the last
      WRITE operation to the storage device.
    </t>
  </section>

  <section anchor='sec:sec_models' title="Security Models">
    <t>
      With loosely coupled storage devices, the metadata server
      uses synthetic uids and gids for the data file, where the uid
      owner of the data file is allowed read/write access and the
      gid owner is allowed read only access.  As part of the layout,
      the client is provided with the rpc credentials to be used
      (see ffm_auth in  <xref target='ff_layout4' />) to access the
      data file.  Fencing off clients is achieved by using SETATTR
      by the server to change the uid and/or gid owners of the data
      file to implicitly revoke the outstanding rpc credentials.
      Note: it is recommended to implement common access control
      methods at the storage device filesystem exports level to
      allow only the metadata server root (super user) access to
      the storage device, and to set the owner of all directories
      holding data files to the root user.  This security method,
      when using weak auth flavors such as AUTH_SYS, provides a
      practical model to enforce access control and fence off
      cooperative clients, but it can not protect against malicious
      clients; hence it provides a level of security equivalent to
      NFSv3.
    </t>

    <t>
      With tightly coupled storage devices, the metadata server
      sets the user and group owners, mode bits, and ACL of the
      data file to be the same as the metadata file. And the client
      must authenticate with the storage device and go through the
      same authorization process it would go through via the metadata
      server.
    </t>
  </section>

  <section title="State and Locking Models">
    <t>
      Metadata file OPEN, LOCK, and DELEGATION operations are always
      executed only against the metadata server.
    </t>

    <t>
      With NFSv4 storage devices, the metadata server, in response to
      the state changing operation, executes them against the respective
      data files on the storage devices.  It then sends the storage
      device open stateid as part of the layout (see the ffm_stateid
      in <xref target='ff_layout4' />) and it is then used by the client
      for executing READ/WRITE operations against the storage device.
    </t>

    <t>
      Standalone NFSv4.1 storage devices that do not return the
      EXCHGID4_FLAG_USE_PNFS_DS flag to EXCHANGE_ID are used the same
      way as NFSv4 storage devices.
    </t>

    <t>
      NFSv4.1 clustered storage devices that do identify themselves
      with the EXCHGID4_FLAG_USE_PNFS_DS flag to EXCHANGE_ID use a
      back-end control protocol as described in
      <xref target="RFC5661" /> to implement a global stateid
      model as defined there.
    </t>
  </section>
</section>
