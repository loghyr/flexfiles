<!-- Copyright (C) The IETF Trust (2014) -->
<!-- Copyright (C) The Internet Society (2014) -->

<section title="Coupling of Storage Devices">
  <t>
    The coupling of the metadata server with the storage devices
    can be either tight or loose. In a tight coupling, there is
    a control protocol present to manage security, LAYOUTCOMMITs,
    etc.  With a loose coupling, the only control protocol might
    be a version of NFS. As such, semantics for managing security,
    state, and locking models MUST be defined.
  </t>

  <section anchor='sec:layoutcommit' title='LAYOUTCOMMIT'>
    <t>
      With a tightly coupled system, when the metadata server
      receives a LAYOUTCOMMIT (see Section 18.42 of <xref
      target='RFC5661' />), the semantics of the File Layout Type
      MUST be met (see Section 12.5.4 of <xref target='RFC5661' />).
      It is the responsibility of the client to make sure the
      data file is stable before the metadata server begins to query
      the storage devices about the changes to the file.  With a
      loosely coupled system, if any WRITE to a storage device did
      not result with stable_how equal to FILE_SYNC, a LAYOUTCOMMIT
      to the metadata server MUST be preceded with a COMMIT to the
      storage device.  Note that if the client has not done a COMMIT
      to the storage device, then the LAYOUTCOMMIT might not be
      synchronized to the last WRITE operation to the storage device.
    </t>
  </section>

  <section anchor='sec:sec_models' title="Fencing Clients from the Data Server">
    <t>
      With loosely coupled storage devices, the metadata server
      uses synthetic uids and gids for the data file, where the uid
      owner of the data file is allowed read/write access and the
      gid owner is allowed read only access.  As part of the layout
      (see ffds_user and ffds_group in  <xref target='ff_layout4' />),
      the client is provided with the user and group to be
      used in the Remote Procedure Call (RPC) <xref target='RFC5531' />
      credentials needed to access the data file.  Fencing off
      of clients is achieved by the metadata server changing the
      synthetic uid and/or gid owners of the data file on the storage
      device to implicitly revoke the outstanding RPC credentials.
    </t>

    <t>
      With this loosely coupled model, the metadata server is 
      not able to fence off a single client, it forced to fence
      off all clients. However, as the other clients react to
      the fencing, returning their layouts and trying to get
      new ones, the metadata server can hand out a new uid and
      gid to allow access.
    </t>

    <t>
      Note: it is recommended to implement common access control
      methods at the storage device filesystem to
      allow only the metadata server root (super user) access to
      the storage device, and to set the owner of all directories
      holding data files to the root user.  This approach
      provides a practical model to enforce access control and fence off
      cooperative clients, but it can not protect against malicious
      clients; hence it provides a level of security equivalent to
      AUTH_SYS.
    </t>

    <t>
      With tightly coupled storage devices, the metadata server
      sets the user and group owners, mode bits, and ACL of the
      data file to be the same as the metadata file. And the client
      must authenticate with the storage device and go through the
      same authorization process it would go through via the metadata
      server.
    </t>

    <section title='Implementation Notes for Synthetic uids/gids'>
      <t>
        The selection method for the synthetic uids and gids to
        be used for fencing in loosely coupled storage devices
        is strictly an implementation issue. An implementation
        might allow an administrator to restrict a range of such
        ids in the name servers.  She might also be able to
        choose an id that would never be used to grant acccess.
        Then when the metadata server had a request to access
        a file, a SETATTR would be sent to the storage device
        to set the owner and group of the data file. The user
        and group might be selected in a round robin fashion
        from the range of available ids.
      </t>

      <t>
        Those ids would be sent back as ffds_user and ffds_group
        to the client. And it would present them as the RPC
        credentials to the storage device. When the client was
        done accessing the file and the metadata server knew
        that no other client was accessing the file, it could
        reset the owner and group to restrict access to the
        data file.
      </t>

      <t>
        When the metadata server wanted to fence off a client,
        it would change the synthetic uid and/or gid to
        the restricted ids. Note that using a restricted id
        ensures that there is a change of owner and at least
        one id available that never gets allowed access.
      </t>
    </section>

    <section title='Example of using Synthetic uids/gids'>
      <t>
	The user loghyr creates a file "ompha.c" on the metadata
	server and it creates a corresponding data file on the
	storage device.
      </t>

      <t>
        The metadata server entry may look like:
      </t>

      <figure>
        <artwork>
-rw-r--r--    1 loghyr  staff    1697 Dec  4 11:31 ompha.c
        </artwork>
      </figure>

      <t>
	On the storage device, it may be assigned some
        random synthetic uid/gid to deny access:
      </t>

      <figure>
        <artwork>
-rw-r-----    1 19452   28418    1697 Dec  4 11:31 data_ompha.c
        </artwork>
      </figure>

      <t>
	When the file is opened on a client, since the layout knows
	nothing about the user (and does not care), whether loghyr
	or garbo opens the file does not matter. The owner and group
	are modified and those values are returned.
      </t>

      <figure>
        <artwork>
-rw-r-----    1 1066    1067     1697 Dec  4 11:31 data_ompha.c
        </artwork>
      </figure>

      <t>
	The set of synthetic gids on the storage device
	should be selected such that there is no mapping in any of
	the name services used by the storage device. I.e., each
	group should have no members.
      </t>

      <t>
	If the layout segment has an iomode of LAYOUTIOMODE4_READ, then
	the metadata server should return a synthetic uid that is
	not set on the storage device. Only the synthetic gid would
	be valid.
      </t>

      <t>
	The client is thus solely responsible for enforcing file
	permissions in a loosely coupled model.  To allow loghyr
	write access, it will send an RPC to the storage device
	with a credential of 1066:1067.  To allow garbo read access,
	it will send an RPC to the storage device with a credential
	of 1067:1067. The value of the uid does not matter as
	long as it is not the synthetic uid granted it when getting
	the layout.
      </t>

      <t>
        While pushing the enforcement of permission checking onto
        the client may seem to weaken security, the client may already
        be responsible for enforcing permissions before modifications
        are sent to a server. With cached writes, the client is
        always responsible for tracking who is modifying a file and
        making sure to not coalesce requests from multiple users
        into one request.
      </t>
    </section>
  </section>

  <section title="State and Locking Models">
    <t>
      Metadata file OPEN, LOCK, and DELEGATION operations are always
      executed only against the metadata server.
    </t>

    <t>
      The metadata server responds to state changing operations by
      executing them against the respective
      data files on the storage devices.  It then sends the storage
      device open stateid as part of the layout (see the ffm_stateid
      in <xref target='ff_layout4' />) and it is then used by the client
      for executing READ/WRITE operations against the storage device.
    </t>

    <t>
      Standalone NFSv4.1+ storage devices that do not return the
      EXCHGID4_FLAG_USE_PNFS_DS flag to EXCHANGE_ID are used the same
      way as NFSv4 storage devices.
    </t>

    <t>
      NFSv4.1+ clustered storage devices that do identify themselves
      with the EXCHGID4_FLAG_USE_PNFS_DS flag to EXCHANGE_ID use a
      back-end control protocol as described in
      <xref target="RFC5661" /> to implement a global stateid
      model as defined there.
    </t>
  </section>
</section>
